{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06a805e6",
   "metadata": {},
   "source": [
    "# Agricultural PDF Processor - Complete Code Explanation\n",
    "\n",
    "## Overview\n",
    "This notebook provides a comprehensive explanation of the Agricultural PDF Processor system, which combines PDF processing, machine learning embeddings, and Azure OpenAI to create an intelligent agricultural knowledge base.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Import Dependencies](#imports)\n",
    "2. [AgriculturalML Class Structure](#class-structure)\n",
    "3. [Database Initialization](#database-init)\n",
    "4. [PDF Processing Pipeline](#pdf-processing)\n",
    "5. [Text Processing & Embeddings](#text-processing)\n",
    "6. [Search & Retrieval](#search-retrieval)\n",
    "7. [Streamlit Interface](#streamlit-interface)\n",
    "8. [Key Features & Workflows](#key-features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847e47d5",
   "metadata": {},
   "source": [
    "## 1. Import Dependencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19afc30c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import sqlite3\n",
    "import pickle\n",
    "import logging\n",
    "import hashlib\n",
    "import fitz  # PyMuPDF\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from openai import AzureOpenAI\n",
    "from typing import List, Dict, Optional\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import streamlit as st\n",
    "from docx import Document\n",
    "from docx.shared import Pt\n",
    "from docx.enum.text import WD_PARAGRAPH_ALIGNMENT\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd5c81c",
   "metadata": {},
   "source": [
    "### Purpose of Each Import:\n",
    "- **os, time**: System operations and timing\n",
    "- **sqlite3**: Local database for storing processed documents\n",
    "- **pickle**: Serializing embeddings for database storage\n",
    "- **logging**: Tracking system operations and errors\n",
    "- **hashlib**: Creating unique identifiers for PDFs and images\n",
    "- **fitz (PyMuPDF)**: PDF processing and text/image extraction\n",
    "- **numpy, pandas**: Data manipulation and analysis\n",
    "- **tqdm**: Progress bars for long operations\n",
    "- **PIL (Pillow)**: Image processing\n",
    "- **openai**: Azure OpenAI API integration\n",
    "- **sentence_transformers**: Creating semantic embeddings\n",
    "- **sklearn**: Similarity calculations\n",
    "- **streamlit**: Web interface\n",
    "- **docx**: Word document operations (future use)\n",
    "- **re**: Regular expressions for text filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812cfabd",
   "metadata": {},
   "source": [
    "## 2. AgriculturalML Class Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfa13c9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class AgriculturalML:\n",
    "    def __init__(self, azure_api_key: str, azure_endpoint: str):\n",
    "        # Initialize Azure OpenAI client\n",
    "        self.client = AzureOpenAI(\n",
    "            api_key=azure_api_key,\n",
    "            api_version=\"2025-01-01-preview\",\n",
    "            azure_endpoint=azure_endpoint,\n",
    "        )\n",
    "\n",
    "        # Initialize embedding model for semantic search\n",
    "        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        \n",
    "        # Database and file management\n",
    "        self.db_path = \"agricultural_ml.db\"\n",
    "        self.image_dir = \"extracted_images\"\n",
    "        os.makedirs(self.image_dir, exist_ok=True)\n",
    "        \n",
    "        # Initialize database schema\n",
    "        self.init_db()\n",
    "        \n",
    "        # Cache for processed PDFs to avoid reprocessing\n",
    "        self.processed_pdfs = set()\n",
    "        self.load_processed_pdfs()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987bade9",
   "metadata": {},
   "source": [
    "### Key Components:\n",
    "1. **Azure OpenAI Client**: Handles AI-powered text enhancement and query expansion\n",
    "2. **Embedding Model**: 'all-MiniLM-L6-v2' creates 384-dimensional vectors for semantic search\n",
    "3. **Database**: SQLite for local storage of processed documents\n",
    "4. **Image Directory**: Stores extracted images from PDFs\n",
    "5. **Processing Cache**: Prevents reprocessing the same PDFs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ab07e0",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Database Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85db889f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def init_db(self):\n",
    "    \"\"\"Initialize database with proper schema\"\"\"\n",
    "    with sqlite3.connect(self.db_path) as conn:\n",
    "        # Create the documents table with ALL required columns\n",
    "        conn.execute('''\n",
    "            CREATE TABLE IF NOT EXISTS documents (\n",
    "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                pdf_hash TEXT,\n",
    "                page_number INTEGER,\n",
    "                content TEXT,\n",
    "                embedding BLOB,\n",
    "                image_paths TEXT,\n",
    "                content_length INTEGER,\n",
    "                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "            )\n",
    "        ''')\n",
    "        \n",
    "        # Create the processed_pdfs table\n",
    "        conn.execute('''\n",
    "            CREATE TABLE IF NOT EXISTS processed_pdfs (\n",
    "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                pdf_hash TEXT UNIQUE,\n",
    "                filename TEXT,\n",
    "                processed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "            )\n",
    "        ''')\n",
    "          # Create the processed_pdfs table\n",
    "        conn.execute('''\n",
    "            CREATE TABLE IF NOT EXISTS processed_pdfs (\n",
    "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                pdf_hash TEXT UNIQUE,\n",
    "                filename TEXT,\n",
    "                processed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "            )\n",
    "        ''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc78167",
   "metadata": {},
   "source": [
    "### Database Schema Explanation:\n",
    "\n",
    "#### Documents Table:\n",
    "- **id**: Primary key for each document chunk\n",
    "- **pdf_hash**: SHA-256 hash of the source PDF (for duplicate detection)\n",
    "- **page_number**: Page number where content was extracted\n",
    "- **content**: Processed text content\n",
    "- **embedding**: Serialized vector representation (384 dimensions)\n",
    "- **image_paths**: Semicolon-separated list of extracted image paths\n",
    "- **content_length**: Length of content (for filtering)\n",
    "- **created_at**: Timestamp of processing\n",
    "\n",
    "#### Processed PDFs Table:\n",
    "- **id**: Primary key\n",
    "- **pdf_hash**: Unique identifier for each PDF\n",
    "- **filename**: Original filename\n",
    "- **processed_at**: Processing timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84784660",
   "metadata": {},
   "source": [
    "### Migration Support:\n",
    "```python\n",
    "# Check if pdf_hash column exists (for migration)\n",
    "cursor = conn.execute(\"PRAGMA table_info(documents)\")\n",
    "columns = [column[1] for column in cursor.fetchall()]\n",
    "\n",
    "if 'pdf_hash' not in columns:\n",
    "    logger.info(\"Adding pdf_hash column to existing documents table...\")\n",
    "    conn.execute('ALTER TABLE documents ADD COLUMN pdf_hash TEXT')\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 4. PDF Processing Pipeline {#pdf-processing}\n",
    "\n",
    "### 4.1 PDF Hash Generation\n",
    "```python\n",
    "def get_pdf_hash(self, pdf_path: str) -> str:\n",
    "    \"\"\"Generate hash for PDF to check if already processed\"\"\"\n",
    "    hasher = hashlib.sha256()\n",
    "    with open(pdf_path, 'rb') as f:\n",
    "        for chunk in iter(lambda: f.read(4096), b\"\"):\n",
    "            hasher.update(chunk)\n",
    "    return hasher.hexdigest()\n",
    "```\n",
    "**Purpose**: Creates unique identifier for each PDF to avoid reprocessing\n",
    "\n",
    "### 4.2 Content Relevance Filtering\n",
    "```python\n",
    "def is_relevant_content(self, text: str) -> bool:\n",
    "    \"\"\"Filter out irrelevant content like headers, footers, page numbers\"\"\"\n",
    "    text = text.strip()\n",
    "    \n",
    "    # Skip if too short\n",
    "    if len(text) < 50:\n",
    "        return False\n",
    "        \n",
    "    # Skip if mostly numbers (likely page numbers, dates, etc.)\n",
    "    if len(re.findall(r'\\d', text)) / len(text) > 0.5:\n",
    "        return False\n",
    "        \n",
    "    # Skip common headers/footers\n",
    "    skip_patterns = [\n",
    "        r'^\\d+$',  # Just page numbers\n",
    "        r'^page \\d+',  # Page headers\n",
    "        r'^\\d+\\s*$',  # Numbers only\n",
    "        r'^(chapter|section)\\s+\\d+$',  # Chapter/section headers only\n",
    "        r'^(figure|table|chart)\\s+\\d+',  # Figure/table captions only\n",
    "        r'^\\w{1,3}\\s*$',  # Very short abbreviations\n",
    "    ]\n",
    "    \n",
    "    for pattern in skip_patterns:\n",
    "        if re.match(pattern, text.lower()):\n",
    "            return False\n",
    "            \n",
    "    return True\n",
    "```\n",
    "**Purpose**: Filters out noise like page numbers, headers, and footers\n",
    "\n",
    "### 4.3 Text Chunking Strategy\n",
    "```python\n",
    "def chunk_text(self, text: str, chunk_size: int = 1000, overlap: int = 200) -> List[str]:\n",
    "    \"\"\"Split long text into overlapping chunks for better embedding\"\"\"\n",
    "    if len(text) <= chunk_size:\n",
    "        return [text]\n",
    "        \n",
    "    chunks = []\n",
    "    start = 0\n",
    "    \n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        \n",
    "        # Try to break at sentence boundary\n",
    "        if end < len(text):\n",
    "            # Look for sentence ending within the last 200 characters\n",
    "            sentence_end = text.rfind('.', start, end)\n",
    "            if sentence_end > start + chunk_size - 200:\n",
    "                end = sentence_end + 1\n",
    "                \n",
    "        chunk = text[start:end].strip()\n",
    "        if chunk and self.is_relevant_content(chunk):\n",
    "            chunks.append(chunk)\n",
    "            \n",
    "        start = end - overlap\n",
    "        \n",
    "    return chunks\n",
    "```\n",
    "**Key Features**:\n",
    "- **Overlap**: 200 characters to maintain context between chunks\n",
    "- **Sentence Boundary**: Breaks at sentence endings when possible\n",
    "- **Relevance Check**: Filters out irrelevant chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab66da49",
   "metadata": {},
   "source": [
    "### 4.4 Main PDF Processing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9c8736",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "```python\n",
    "def process_pdf(self, pdf_path: str, force_reprocess: bool = False):\n",
    "    \"\"\"Process PDF only if not already processed\"\"\"\n",
    "    pdf_hash = self.get_pdf_hash(pdf_path)\n",
    "    \n",
    "    if not force_reprocess and pdf_hash in self.processed_pdfs:\n",
    "        logger.info(f\"PDF already processed (hash: {pdf_hash[:8]})\")\n",
    "        return pdf_hash\n",
    "        \n",
    "    logger.info(f\"Processing new PDF (hash: {pdf_hash[:8]})\")\n",
    "    \n",
    "    doc = fitz.open(pdf_path)\n",
    "    all_text_data = []\n",
    "\n",
    "    for page_num in tqdm(range(len(doc)), desc=\"Processing PDF pages\"):\n",
    "        page = doc[page_num]\n",
    "        text = page.get_text(\"text\")\n",
    "        \n",
    "        # Skip if no meaningful content\n",
    "        if not self.is_relevant_content(text):\n",
    "            continue\n",
    "            \n",
    "        # Extract images\n",
    "        images = page.get_images(full=True)\n",
    "        image_paths = []\n",
    "        \n",
    "        for img_index, img in enumerate(images):\n",
    "            try:\n",
    "                xref = img[0]\n",
    "                base_image = doc.extract_image(xref)\n",
    "                image_bytes = base_image[\"image\"]\n",
    "                image_ext = base_image[\"ext\"]\n",
    "                image_hash = hashlib.sha1(image_bytes).hexdigest()\n",
    "                image_filename = os.path.join(\n",
    "                    self.image_dir, \n",
    "                    f\"{pdf_hash[:8]}_page{page_num+1}_img{img_index+1}_{image_hash[:8]}.{image_ext}\"\n",
    "                )\n",
    "                \n",
    "                # Avoid duplicate images\n",
    "                if not os.path.exists(image_filename):\n",
    "                    with open(image_filename, \"wb\") as f:\n",
    "                        f.write(image_bytes)\n",
    "                image_paths.append(image_filename)\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Failed to extract image {img_index} from page {page_num}: {e}\")\n",
    "\n",
    "        self.store_document(pdf_hash, page_num + 1, text, image_paths)\n",
    "        all_text_data.append({\"page\": page_num + 1, \"content\": text})\n",
    "\n",
    "    # Mark PDF as processed\n",
    "    with sqlite3.connect(self.db_path) as conn:\n",
    "        conn.execute('''\n",
    "            INSERT OR REPLACE INTO processed_pdfs (pdf_hash, filename)\n",
    "            VALUES (?, ?)\n",
    "        ''', (pdf_hash, os.path.basename(pdf_path)))\n",
    "        \n",
    "    self.processed_pdfs.add(pdf_hash)\n",
    "\n",
    "    doc.close()\n",
    "    return pdf_hash\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ee7c79",
   "metadata": {},
   "source": [
    "**Processing Steps**:\n",
    "1. **Hash Check**: Verify if PDF already processed\n",
    "2. **Text Extraction**: Extract text from each page\n",
    "3. **Image Extraction**: Extract and save images with unique names\n",
    "4. **Content Storage**: Store text chunks and metadata\n",
    "5. **Cache Update**: Mark PDF as processed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca4b24b",
   "metadata": {},
   "source": [
    "## 5. Text Processing & Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dca7936",
   "metadata": {},
   "source": [
    "### 5.1 Embedding Generation\n",
    "```python\n",
    "def generate_embeddings(self, texts: List[str]) -> np.ndarray:\n",
    "    return self.embedding_model.encode(texts)\n",
    "```\n",
    "**Purpose**: Converts text into 384-dimensional vectors for semantic search\n",
    "\n",
    "### 5.2 Text Beautification\n",
    "```python\n",
    "def beautify_text(self, text: str) -> str:\n",
    "    prompt = f\"\"\"\n",
    "Enhance the following agricultural document content for clarity and professionalism while maintaining all key information:\n",
    "\n",
    "{text}\n",
    "\n",
    "Enhanced version:\n",
    "\"\"\"\n",
    "    try:\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.3,\n",
    "            max_tokens=1000\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Beautification error: {e}\")\n",
    "        return text\n",
    "```\n",
    "**Purpose**: Uses Azure OpenAI to enhance text clarity and professionalism\n",
    "\n",
    "### 5.3 Query Enhancement\n",
    "```python\n",
    "def enhance_query(self, query: str) -> str:\n",
    "    prompt = f\"\"\"\n",
    "You are a helpful assistant improving search queries for agricultural document retrieval. \n",
    "Expand the following query with relevant agricultural terms and context:\n",
    "\n",
    "Original query: {query}\n",
    "\n",
    "Enhanced query with agricultural context:\n",
    "\"\"\"\n",
    "    try:\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.3,\n",
    "            max_tokens=200\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Query enhancement error: {e}\")\n",
    "        return query\n",
    "```\n",
    "**Purpose**: Expands user queries with relevant agricultural terminology\n",
    "\n",
    "### 5.4 Document Storage\n",
    "```python\n",
    "def store_document(self, pdf_hash: str, page_number: int, content: str, image_paths: List[str]):\n",
    "    \"\"\"Store document chunks with PDF hash\"\"\"\n",
    "    chunks = self.chunk_text(content)\n",
    "    \n",
    "    if not chunks:\n",
    "        return\n",
    "        \n",
    "    embeddings = self.generate_embeddings(chunks)\n",
    "    image_paths_str = ';'.join(image_paths)\n",
    "    \n",
    "    with sqlite3.connect(self.db_path) as conn:\n",
    "        for i, (chunk, embedding) in enumerate(zip(chunks, embeddings)):\n",
    "            embedding_blob = pickle.dumps(embedding)\n",
    "            conn.execute('''\n",
    "                INSERT INTO documents (pdf_hash, page_number, content, embedding, image_paths, content_length)\n",
    "                VALUES (?, ?, ?, ?, ?, ?)\n",
    "            ''', (pdf_hash, page_number, chunk, embedding_blob, image_paths_str, len(chunk)))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7f3888",
   "metadata": {},
   "source": [
    "**Process**:\n",
    "1. **Chunk Text**: Split into manageable pieces\n",
    "2. **Generate Embeddings**: Create vectors for each chunk\n",
    "3. **Serialize**: Convert embeddings to binary format\n",
    "4. **Store**: Save to database with metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb61220f",
   "metadata": {},
   "source": [
    "# 6. Search & Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4568ca",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def similarity_search(self, query: str, top_k: int = 5, similarity_threshold: float = 0.3) -> List[Dict]:\n",
    "    \"\"\"Enhanced similarity search with filtering\"\"\"\n",
    "    improved_query = self.enhance_query(query)\n",
    "    query_embedding = self.generate_embeddings([improved_query])[0]\n",
    "    \n",
    "    with sqlite3.connect(self.db_path) as conn:\n",
    "        cursor = conn.execute('''\n",
    "            SELECT id, pdf_hash, page_number, content, embedding, image_paths, content_length \n",
    "            FROM documents \n",
    "            WHERE content_length > 50\n",
    "            ORDER BY content_length DESC\n",
    "        ''')\n",
    "        \n",
    "        results = []\n",
    "        seen_content = set()  # Avoid duplicate content\n",
    "        \n",
    "        for doc_id, pdf_hash, page_number, content, embedding_blob, image_paths_str, content_length in cursor.fetchall():\n",
    "            # Skip if very similar content already seen\n",
    "            content_hash = hashlib.md5(content.encode()).hexdigest()\n",
    "            if content_hash in seen_content:\n",
    "                continue\n",
    "            seen_content.add(content_hash)\n",
    "            \n",
    "            doc_embedding = pickle.loads(embedding_blob)\n",
    "            similarity = cosine_similarity([query_embedding], [doc_embedding])[0][0]\n",
    "            \n",
    "            # Apply similarity threshold\n",
    "            if similarity >= similarity_threshold:\n",
    "                results.append({\n",
    "                    'id': doc_id,\n",
    "                    'pdf_hash': pdf_hash,\n",
    "                    'page_number': page_number,\n",
    "                    'content': content,\n",
    "                    'image_paths': image_paths_str.split(';') if image_paths_str else [],\n",
    "                    'similarity': float(similarity),\n",
    "                    'content_length': content_length\n",
    "                })\n",
    "    \n",
    "    # Sort by similarity and content length\n",
    "    results.sort(key=lambda x: (x['similarity'], x['content_length']), reverse=True)\n",
    "    return results[:top_k]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fcbbb4",
   "metadata": {},
   "source": [
    "**Search Algorithm**:\n",
    "1. **Query Enhancement**: Expand with agricultural terms\n",
    "2. **Embedding Generation**: Convert query to vector\n",
    "3. **Database Retrieval**: Get all documents with sufficient content\n",
    "4. **Similarity Calculation**: Compare query vector with document vectors\n",
    "5. **Filtering**: Apply similarity threshold and remove duplicates\n",
    "6. **Ranking**: Sort by similarity and content length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd10eb89",
   "metadata": {},
   "source": [
    "### 6.2 Search and Beautify\n",
    "```python\n",
    "def search_and_beautify(self, query: str, top_k: int = 5) -> List[Dict]:\n",
    "    \"\"\"Search and beautify results with better filtering\"\"\"\n",
    "    results = self.similarity_search(query, top_k)\n",
    "    beautified_results = []\n",
    "    \n",
    "    for result in results:\n",
    "        beautified_text = self.beautify_text(result['content'])\n",
    "        beautified_results.append({\n",
    "            'pdf_hash': result['pdf_hash'],\n",
    "            'page_number': result['page_number'],\n",
    "            'original_text': result['content'],\n",
    "            'beautified_text': beautified_text,\n",
    "            'image_paths': result['image_paths'],\n",
    "            'similarity_score': result['similarity']\n",
    "        })\n",
    "        \n",
    "    return beautified_results\n",
    "```\n",
    "**Purpose**: Combines search with AI-powered text enhancement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5543d403",
   "metadata": {},
   "source": [
    "## 7. Streamlit Interface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b07aa58",
   "metadata": {},
   "source": [
    "### 7.1 Main Application Structure\n",
    "```python\n",
    "def main():\n",
    "    st.set_page_config(page_title=\"Agricultural PDF Processor\", page_icon=\"üìÑ\", layout=\"wide\")\n",
    "    \n",
    "    # Initialize session state\n",
    "    if 'expanded_content' not in st.session_state:\n",
    "        st.session_state.expanded_content = {}\n",
    "    if 'search_results' not in st.session_state:\n",
    "        st.session_state.search_results = None\n",
    "```\n",
    "\n",
    "### 7.2 Configuration Sidebar\n",
    "```python\n",
    "with st.sidebar:\n",
    "    st.header(\"Configuration\")\n",
    "    azure_api_key = st.text_input(\"Azure API Key\", type=\"password\")\n",
    "    azure_endpoint = st.text_input(\"Azure Endpoint\")\n",
    "    \n",
    "    st.header(\"Search Settings\")\n",
    "    top_k = st.slider(\"Number of results\", 1, 20, 5)\n",
    "    similarity_threshold = st.slider(\"Similarity threshold\", 0.0, 1.0, 0.3, 0.1)\n",
    "```\n",
    "\n",
    "### 7.3 PDF Processing Interface\n",
    "```python\n",
    "col1, col2 = st.columns([1, 1])\n",
    "\n",
    "with col1:\n",
    "    st.header(\"PDF Processing\")\n",
    "    uploaded_files = st.file_uploader(\"Upload PDF files\", type=\"pdf\", accept_multiple_files=True)\n",
    "    force_reprocess = st.checkbox(\"Force reprocess (ignore cache)\")\n",
    "    \n",
    "    if uploaded_files and azure_api_key and azure_endpoint:\n",
    "        ml_system = AgriculturalML(azure_api_key, azure_endpoint)\n",
    "        \n",
    "        # Show current status\n",
    "        status = ml_system.get_processing_status()\n",
    "        st.info(f\"Database contains {status['processed_pdfs']} processed PDFs with {status['document_chunks']} document chunks\")\n",
    "        \n",
    "        if st.button(\"Process PDFs\"):\n",
    "            progress_bar = st.progress(0)\n",
    "            status_text = st.empty()\n",
    "            \n",
    "            for i, uploaded_file in enumerate(uploaded_files):\n",
    "                status_text.text(f\"Processing {uploaded_file.name}...\")\n",
    "                \n",
    "                # Save uploaded file\n",
    "                temp_path = f\"temp_{uploaded_file.name}\"\n",
    "                with open(temp_path, \"wb\") as f:\n",
    "                    f.write(uploaded_file.read())\n",
    "                \n",
    "                try:\n",
    "                    pdf_hash = ml_system.process_pdf(temp_path, force_reprocess)\n",
    "                    st.success(f\"‚úÖ Processed {uploaded_file.name} (hash: {pdf_hash[:8]})\")\n",
    "                except Exception as e:\n",
    "                    st.error(f\"‚ùå Error processing {uploaded_file.name}: {str(e)}\")\n",
    "                finally:\n",
    "                    # Clean up temp file\n",
    "                    if os.path.exists(temp_path):\n",
    "                        os.remove(temp_path)\n",
    "                \n",
    "                progress_bar.progress((i + 1) / len(uploaded_files))\n",
    "```\n",
    "\n",
    "### 7.4 Search Interface\n",
    "```python\n",
    "with col2:\n",
    "    st.header(\"Search & Query\")\n",
    "    query = st.text_input(\"üîç Enter your search query:\")\n",
    "    \n",
    "    if query and azure_api_key and azure_endpoint:\n",
    "        if st.button(\"üîç Search\"):\n",
    "            with st.spinner(\"Searching...\"):\n",
    "                if ml_system is None:\n",
    "                    ml_system = AgriculturalML(azure_api_key, azure_endpoint)\n",
    "                results = ml_system.search_and_beautify(query, top_k)\n",
    "                \n",
    "                if results:\n",
    "                    st.session_state.search_results = results\n",
    "                    st.success(f\"Found {len(results)} relevant results\")\n",
    "                else:\n",
    "                    st.session_state.search_results = None\n",
    "                    st.warning(\"No relevant results found. Try adjusting your query or lowering the similarity threshold.\")\n",
    "```\n",
    "\n",
    "### 7.5 Results Display\n",
    "```python\n",
    "if st.session_state.search_results:\n",
    "    for i, result in enumerate(st.session_state.search_results, 1):\n",
    "        with st.expander(f\"üìÑ Result {i} - Page {result['page_number']} (Similarity: {result['similarity_score']:.3f})\"):\n",
    "            st.markdown(\"**Enhanced Content:**\")\n",
    "            st.markdown(result['beautified_text'])\n",
    "            \n",
    "            if st.checkbox(f\"Show original text {i}\", key=f\"original_{i}\"):\n",
    "                st.markdown(\"**Original Content:**\")\n",
    "                st.text(result['original_text'])\n",
    "            \n",
    "            # Display images\n",
    "            for image_path in result['image_paths']:\n",
    "                if os.path.exists(image_path):\n",
    "                    st.image(image_path, width=400, caption=f\"Image from page {result['page_number']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c26a1e",
   "metadata": {},
   "source": [
    "## 8. Key Features & Workflows\n",
    "\n",
    "### 8.1 Intelligent PDF Processing\n",
    "- **Duplicate Detection**: Uses SHA-256 hashing to avoid reprocessing\n",
    "- **Content Filtering**: Removes headers, footers, and irrelevant content\n",
    "- **Image Extraction**: Saves images with unique identifiers\n",
    "- **Text Chunking**: Splits content optimally for embedding\n",
    "\n",
    "### 8.2 Semantic Search\n",
    "- **Query Enhancement**: Expands user queries with agricultural context\n",
    "- **Vector Similarity**: Uses cosine similarity for semantic matching\n",
    "- **Relevance Filtering**: Applies configurable similarity thresholds\n",
    "- **Duplicate Removal**: Prevents duplicate content in results\n",
    "\n",
    "### 8.3 AI-Powered Enhancement\n",
    "- **Text Beautification**: Improves clarity and professionalism\n",
    "- **Context Expansion**: Provides detailed agricultural insights\n",
    "- **Error Handling**: Graceful fallbacks for API failures\n",
    "\n",
    "### 8.4 User Experience\n",
    "- **Progress Tracking**: Real-time processing updates\n",
    "- **Configurable Parameters**: Adjustable search settings\n",
    "- **Visual Results**: Displays images alongside text\n",
    "- **Session Management**: Maintains state across interactions\n",
    "\n",
    "### 8.5 Data Management\n",
    "- **SQLite Database**: Efficient local storage\n",
    "- **Embedding Serialization**: Compact vector storage\n",
    "- **Migration Support**: Handles schema updates\n",
    "- **Status Monitoring**: Tracks processing statistics"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
